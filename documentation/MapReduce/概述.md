# Task运行过程

MapReduce中，一个应用程序被分为Map和Reduce两个计算阶段，分别有一个或多个Map Task和Reduce Task组成。

其中，每个Map Task处理输入数据集合中的一个InputSplit，并将产生的若干数据片段写入本地磁盘中，而Reduce Task则从每个Map Task中远程拷贝相应的数据片段，经分组聚集和归约后，将结果写到HDFS上作为最终结果。Map Task和Reduce Task之间的数据传输采用pull模型。为了容错，Map Task将中间计算结果存放在本地磁盘上，而Reduce Task则通过HTTP请求从各个Map Task端pull相应的输入数据。为支持大量Reduce Task并发从Map Task端拷贝数据，Hadoop采用Jetty Server作为HTTP Server处理并发数据读请求。

Map Task执行过程可概括为：首先通过用户提供的InputFormat将对应的InputSplit解析成一系列的key/value，依次交给用户编写的map()函数处理；接着按照指定的Partitioner对数据分片，确定每个key/value将交由哪个Reduce Task处理；若用户定义了Combiner，则数据将交由Combiner进行一次本的reduce操作；最后将结果保存到本地磁盘上。

Reduce Task首先通过HTTP请求从各个已经运行完成的Map Task上拷贝对应的数据分片，待所有数据拷贝完成后，再以key为关键字对所有数据进行排序，通过排序，key相同的记录聚集到一起形成若干分组，然后将每组数据交由用户编写的reduce()函数处理，并将数据结果直接写到

# 基本数据结构和算法